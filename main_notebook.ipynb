{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRC_f1mqv0eE"
   },
   "source": [
    "# Outline\n",
    "- [ 1 - Import Packages](#1)\n",
    "- [ 2 - Prepare dataset](#2)\n",
    "  - [ 2.1 - Missing Values](#2.1)\n",
    "  - [ 2.2 - Zero Values](#2.2)\n",
    "  - [ 2.3 - **Number of months** entries avaliable of each **Stock Factor**](#2.3)\n",
    "  - [ 2.4 - Clean data](#2.4)\n",
    "  - [ 2.5 - Extract Factors and Stocks](#2.5)\n",
    "      - [ 2.5.1 - **Heterogeneous ensembles** approach](#2.5.1)\n",
    "      - [ 2.5.2 - RFE with `n_estimators=500`](#2.5.2)\n",
    "      - [ 2.5.3 - Evaluation for Each Technique](#2.5.3)\n",
    "  - [ 2.6 - Ramdom sample (for testing)](#2.6)\n",
    "- [ 3 - Run Prediction](#3)\n",
    "- [ 4 - Models Evaluation](#4)\n",
    "- [ 5 - Cumulative Performance](#5)\n",
    "  - [ 5.1 - Plot Cumulative Performance: Mixed Strategy vs. S&P 500 (2010â€“2023)](#5.1)\n",
    "  - [ 5.2 - 10 most held stocks in our portfolio](#5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: `3.12.8 (tags/v3.12.8:2dc476b, Dec  3 2024, 19:30:04) [MSC v.1942 64 bit (AMD64)]`\n",
      "Base Python location: `C:\\Users\\LMT\\AppData\\Local\\Programs\\Python\\Python312`\n",
      "Current Environment location: `.venv_investment_allocation`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python Version: `{sys.version}`\")  # Detailed version info\n",
    "print(f\"Base Python location: `{sys.base_prefix}`\")\n",
    "print(f\"Current Environment location: `{os.path.basename(sys.prefix)}`\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgsZ8tpKwEYI"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Packages\n",
    "\n",
    "Run the cell below to import all the necessary packages: <br>\n",
    "`pip install -r requirements.txt` or `pip install pandas scikit-learn statsmodels xgboost matplotlib requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "On15w27TBKQJ"
   },
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt\n",
    "# %pip install pandas scikit-learn statsmodels xgboost matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cYQ-ZPHowTIj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.feature_selection import mutual_info_regression, RFE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NC9svGONwTIi"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd \"/content/drive/MyDrive/Hackathon\"\n",
    "\n",
    "# Define paths and helper functions\n",
    "ASSET_FOLDER = \"asset\"\n",
    "ASSET_FACTOR_PATH = os.path.join(ASSET_FOLDER, 'factor_char_list.csv')\n",
    "ASSET_DATA_PATH = os.path.join(ASSET_FOLDER, 'hackathon_sample_v2.csv')\n",
    "ASSET_MKT_IND_PATH = os.path.join(ASSET_FOLDER, 'mkt_ind.csv')\n",
    "os.makedirs(ASSET_FOLDER, exist_ok=True)\n",
    "\n",
    "CLEAN_DATA_FOLDER = \"clean_data\"\n",
    "CLEAN_FACTOR_PATH = os.path.join(CLEAN_DATA_FOLDER, 'factor.csv')\n",
    "CLEAN_DATA_PATH = os.path.join(CLEAN_DATA_FOLDER, 'data.csv')\n",
    "os.makedirs(CLEAN_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "PREDICTED_FOLDER = \"predictions\"\n",
    "OUTPUT_PREDICTS_PATH = os.path.join(PREDICTED_FOLDER, 'output.csv')\n",
    "os.makedirs(PREDICTED_FOLDER, exist_ok=True)\n",
    "\n",
    "def save_file(df, file_name='file.csv', with_index=False):\n",
    "    df.to_csv(file_name, index=with_index)\n",
    "    print(f\"Saved `{file_name}`.\")\n",
    "\n",
    "def read_file(file_name='file.csv', parse_dates=[]):\n",
    "    print(f\"Read `{file_name}`.\")\n",
    "    return pd.read_csv(file_name, parse_dates=parse_dates)\n",
    "\n",
    "def inputData(factor_file=CLEAN_FACTOR_PATH, data_file=CLEAN_DATA_PATH):\n",
    "    factor = list(read_file(factor_file)[\"variable\"].values)\n",
    "    data = read_file(data_file, parse_dates=['date'])\n",
    "    return factor, data\n",
    "\n",
    "def outputData(factor, data, factor_file=CLEAN_FACTOR_PATH, data_file=CLEAN_DATA_PATH):\n",
    "    save_file(pd.DataFrame({'variable': factor}), factor_file)\n",
    "    save_file(data, data_file)\n",
    "\n",
    "def download_file(url, save_path, extract=False):\n",
    "    \"\"\"Download a file and optionally extract if it's compressed\"\"\"\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    save_path += '.zip' if extract else ''\n",
    "    \n",
    "    print(f\"Downloading `{save_path}`... \", end='')\n",
    "    \n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"| Failed: [{response.reason}]\")\n",
    "        return response\n",
    "        \n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Extract if needed\n",
    "    if extract:\n",
    "        try:\n",
    "            shutil.unpack_archive(save_path, extract_dir=os.path.dirname(save_path))\n",
    "            print(f\"| Extracted archive \", end='')\n",
    "            # Clean up the archive file after extraction\n",
    "            os.remove(save_path)\n",
    "        except shutil.ReadError:\n",
    "            print(f\"| Note: {save_path} is not an archive or could not be extracted \", end='')\n",
    "    \n",
    "    print(f\"| Successed\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHxG4aE-wY0Z"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading `asset\\factor_char_list.csv`... | Completed\n",
      "Downloading `asset\\hackathon_sample_v2.csv.zip`... | Extracted archive | Completed\n",
      "Downloading `asset\\mkt_ind.csv`... | Completed\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Kaggle\n",
    "BASE_URL = \"https://www.kaggle.com/api/v1/datasets/download/minhthonglai/mcgill-fiam-asset-management-hackathon\"\n",
    "\n",
    "# Factor char list\n",
    "download_file(\n",
    "    f'{BASE_URL}/factor_char_list.csv',\n",
    "    ASSET_FACTOR_PATH\n",
    ")\n",
    "\n",
    "# Hackathon sample - with potential extraction\n",
    "download_file(\n",
    "    f'{BASE_URL}/hackathon_sample_v2.csv',\n",
    "    ASSET_DATA_PATH,\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "# Market index\n",
    "response = download_file(\n",
    "    f'{BASE_URL}/mkt_ind.csv',\n",
    "    ASSET_MKT_IND_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfbW1wq4eT8X"
   },
   "source": [
    "#### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqHYcb8wl3FF",
    "outputId": "4a89263c-dda3-4283-96ef-3469a16e0aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read `asset\\factor_char_list.csv`.\n",
      "Read `asset\\hackathon_sample_v2.csv`.\n"
     ]
    }
   ],
   "source": [
    "stock_vars, raw = inputData(factor_file=ASSET_FACTOR_PATH, data_file=ASSET_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpQOk2xS2bwJ",
    "outputId": "7d933b2e-ed2f-44ee-c110-888f6cae9da6"
   },
   "outputs": [],
   "source": [
    "raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHuJqfOfugsM"
   },
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### 2.1 - Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us4RMX7NTz9m"
   },
   "source": [
    "- For **factor/stock factor selection**, factors with too many **zero** values it may indicate that the factor is not informative.\n",
    "- In such cases, it might be beneficial to analyze the factor's distribution and consider whether it adds value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NecGFMI_Q1U1",
    "outputId": "eaa6529b-3d8a-4ff0-fe4f-0084d60aac18"
   },
   "outputs": [],
   "source": [
    "missing_values = raw[stock_vars].isnull().sum().rename('missing_values')\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UUBmAPYmJy2",
    "outputId": "f817a881-bd97-42c0-fd7d-352cb0099f51"
   },
   "outputs": [],
   "source": [
    "limit = 30\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "missing_values[:limit].plot(\n",
    "    kind='bar',\n",
    "    color='skyblue',\n",
    "    title=f'{limit} Highest Factors Have Missing Values',\n",
    "    xlabel='Stock Factors',\n",
    "    ylabel='Number of Missing Values',\n",
    "    rot=45)\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for index, value in enumerate(missing_values[:limit]):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONc_BMHvP36x",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### 2.2 - Zero Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhXQs8Q6apM5"
   },
   "source": [
    "- For **factor/stock factor selection**, factors with too many missing values can lead to biased or unreliable model predictions.\n",
    "- Factors with **fewer missing values** can be retained and addressed later using imputation methods to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dWpqNPrWRGvv",
    "outputId": "fa9a5e7c-6ec3-4eb2-86e9-9c266e8e98e3"
   },
   "outputs": [],
   "source": [
    "zero_values = raw[stock_vars][raw[stock_vars] == 0].count().rename('zero_values')\n",
    "zero_values = zero_values[zero_values > 0].sort_values(ascending=False)\n",
    "zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJI1mt98QzI0",
    "outputId": "06d346bd-1cd8-453f-b07d-ff549e6f5e23"
   },
   "outputs": [],
   "source": [
    "limit = 30\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "zero_values[:limit].plot(\n",
    "    kind='bar',\n",
    "    color='skyblue',\n",
    "    title=f'{limit} Highest Total Zeros Values of Stock Factors',\n",
    "    xlabel='Stock Factors',\n",
    "    ylabel='Total Zeros',\n",
    "    rot=45,\n",
    "    )\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for index, value in enumerate(zero_values[:limit]):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM4ROZm_QAYy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 - **Number of months** entries avaliable of each **Stock Factor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Adur4oSRrqz"
   },
   "source": [
    "- The sample dataset spans **24 years (288 months) from 2000-2024**. However, some stocks do not cover the entire **288-month** period.\n",
    "- Therefore, selecting stocks with the **highest totals of available months** in the dataset would enhance the models' ability to generalize predictions effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBBHLptuRjjA",
    "outputId": "09a2e404-969f-4490-8a9e-0de198e4f2d0"
   },
   "outputs": [],
   "source": [
    "month_counts_by_stock = raw[['permno', 'month']].set_index('month').value_counts().rename('month_count')\n",
    "stock_counts_by_amount_of_months = month_counts_by_stock.value_counts().rename('stock_count').sort_index(ascending=False)\n",
    "stock_counts_by_amount_of_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoGYMQ3ePrhk",
    "outputId": "2fa0749a-895c-43cf-9468-be7cd0f1fb86"
   },
   "outputs": [],
   "source": [
    "limit = 10\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "stock_counts_by_amount_of_months[:limit].plot(\n",
    "    kind='bar',\n",
    "    color='skyblue',\n",
    "    title=f'Total Stocks in the {limit} Highest Amount of Months Avaliable in data',\n",
    "    xlabel='Amount of Months Avaliable',\n",
    "    ylabel='Total Stocks',\n",
    "    rot=45)\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for index, value in enumerate(stock_counts_by_amount_of_months[:limit]):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vHdjIe7R5_i"
   },
   "source": [
    "<a name=\"2.4\"></a>\n",
    "### 2.4 - Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li6O9XksfsFS"
   },
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LzLzTXOFA7L"
   },
   "source": [
    "To ensure the integrity of the dataset, we will implement a function that cleans the data. This function will focus on selecting factors and stocks based on specific criteria regarding missing and zero values.\n",
    "\n",
    "Select **factors** that has:\n",
    "- *Missing Values:* **Least** amount of **missing values** keep factors with less than **30%** missing values.\n",
    "- *Zero Values:* **Least** amount of **zeros values**: keep factors with less than **20%** zero values\n",
    "\n",
    "Select **stocks** that has:\n",
    "- *Available Months:* **Least** amount of **missing months**(**Highest** amount of **available months**)\n",
    "- *All Missing Values:* **Remove** stocks that have factor(s) contains all missing values\n",
    "\n",
    "Fill missing values with special methods (Drop stocks one that have all missing values):\n",
    "  - **Mean/Median:** ```For smaller gaps, you can fill missing values with the mean or median of the available data. This method is simple but may not capture the temporal dynamics of the data effectively.```\n",
    "\n",
    "**Ranking** and **Normalization**: Implements the vectorized approach for ranking (was a part in provided file `penalized_linear_hackathon.py`.)\n",
    "- *Ranking*: assign a rank to each factor based on its values, which can be crucial for understanding the relative performance of different stocks.\n",
    "- *Normalization*: adjust the values in the dataset to a common scale, which is essential for ensuring that the ranking is not biased by the scale of the factors, ensuring that all factors contribute equally to the ranking process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_xyF0lEsF9S"
   },
   "source": [
    "#### Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d7iS3RELEzP"
   },
   "outputs": [],
   "source": [
    "def cleandata(input_factor: list,\n",
    "              raw: pd.DataFrame,\n",
    "              missing_values_percent_threshold=0.30, # Keep factors that have lower missing values percentage than this\n",
    "              zero_values_percent_threshold=0.20, # Keep factors that have lower zero values percentage than this\n",
    "              months_threshold=100 # Keep stocks that have higher number of months data than this\n",
    "              ) -> pd.DataFrame:\n",
    "\n",
    "  clean_data = raw[input_factor]\n",
    "  total_entries = len(clean_data)\n",
    "  left_hand_side_vars = ['year', 'month', 'date', 'permno', 'comp_name', 'stock_exret'] # those are not part of the factors, but should be kept\n",
    "\n",
    "\n",
    "\n",
    "  # SELECT FACTORS\n",
    "  # Get least missing values factors (30% threshold default)\n",
    "  missing_values = clean_data.isnull().sum()\n",
    "  least_missing_factors = missing_values[missing_values < total_entries * missing_values_percent_threshold].index\n",
    "\n",
    "  # Get least zero values factors (20% threshold default)\n",
    "  zero_values = (clean_data == 0).sum()\n",
    "  least_zero_factors = zero_values[zero_values < total_entries * zero_values_percent_threshold].index\n",
    "  clean_factor = list(set(least_missing_factors) & set(least_zero_factors))\n",
    "  clean_data = clean_data[clean_factor]\n",
    "\n",
    "\n",
    "\n",
    "  # SELECT STOCKS\n",
    "  # Merge data with `left_hand_side_vars` from `raw`\n",
    "  clean_data = pd.concat([raw[left_hand_side_vars], clean_data], axis=1)\n",
    "\n",
    "  # Remove stocks that have factor(s) containing all missing values in any factor\n",
    "  clean_data = clean_data.groupby('permno').filter(lambda x: not x[clean_factor].isnull().all().any())\n",
    "\n",
    "  # Select stocks that have least missing\n",
    "  month_counts_by_stock = clean_data[['permno', 'month']].set_index('month').value_counts().rename('month_count') #value_counts() already sort\n",
    "  select_permno = month_counts_by_stock[month_counts_by_stock >= months_threshold].reset_index()['permno']\n",
    "  clean_data = clean_data[clean_data['permno'].isin(select_permno)]\n",
    "\n",
    "\n",
    "\n",
    "  # FILLING MISSING VALUES\n",
    "  # Calculate the median for each factor and fill missing values\n",
    "  medians = clean_data.groupby('permno')[clean_factor].transform('median')\n",
    "\n",
    "  # Fill missing values with the corresponding median\n",
    "  clean_data[clean_factor] = clean_data[clean_factor].fillna(medians)\n",
    "\n",
    "  clean_data[clean_factor] = clean_data[clean_factor].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # RANKING AND NORMALIZATION\n",
    "  # Rank all selected factors for each stock (permno)\n",
    "  clean_data[clean_factor] = clean_data.groupby('permno')[clean_factor].rank(method=\"dense\") - 1\n",
    "  max_ranks = clean_data.groupby('permno')[clean_factor].transform('max')\n",
    "\n",
    "  # Normalize the ranked values to the range [-1, 1]\n",
    "  normalized_data = (clean_data[clean_factor] / max_ranks) * 2 - 1\n",
    "  normalized_data[max_ranks == 0] = 0   # Avoid division by zero by checking where max_ranks == 0 and setting those values to 0\n",
    "  clean_data[clean_factor] = normalized_data\n",
    "\n",
    "\n",
    "  return clean_factor, clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9gTydau4ELM",
    "outputId": "8a32ae64-426f-47bd-a905-120d23760956"
   },
   "outputs": [],
   "source": [
    "factor, data = cleandata(stock_vars, raw)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXSay8bUKScm",
    "outputId": "1c899c0a-3166-497d-e5e2-3a33a8a8dc0f"
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "outputData(factor, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCR0f2xWSHPS"
   },
   "source": [
    "#### Cleaned data subset that contains stocks have full 288 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc0vzhoTwTIj"
   },
   "outputs": [],
   "source": [
    "# factors_288_months, data_288_months = cleandata(stock_vars, raw, months_threshold=288)\n",
    "# data_288_months.info()\n",
    "# outputData(factors_288_months, data_288_months, data_file=os.path.join(CLEAN_DATA_FOLDER, 'data_288_months.csv'), factor_file=os.path.join(CLEAN_DATA_FOLDER, 'factor_288_months.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IljUjSBbwTIk"
   },
   "source": [
    "<a name=\"2.5\"></a>\n",
    "### 2.5 - Extract Factors and Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmQuEZbfeT8c"
   },
   "source": [
    "We experimented with two different approaches: Heterogeneous Ensembles and RFE with n_estimators=500. Ultimately, we chose the one that produced the most favorable results.\n",
    "\n",
    "Feature Selection Techniques\n",
    "<div style=\"text-align:center\"><img src=\"https://www.mdpi.com/logistics/logistics-05-00080/article_deploy/html/images/logistics-05-00080-g003.png\" width=\"70%\"></div>\n",
    "\n",
    "*Source: [Survey of feature selection and extraction techniques for stock market prediction](https://jfin-swufe.springeropen.com/articles/10.1186/s40854-022-00441-7#Sec23)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do4AJl_5eT8d",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Extracting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZ2eEQYceT8d"
   },
   "outputs": [],
   "source": [
    "def load_and_extract_data(data: pd.DataFrame,\n",
    "                          selected_factors=[],\n",
    "                          selected_stocks=[],\n",
    "                          rand_factors=False,\n",
    "                          rand_stocks=False):\n",
    "\n",
    "  stock_permnos = data['permno'].unique().tolist()\n",
    "\n",
    "  # Selecting random factors\n",
    "  if rand_factors:\n",
    "    num_factors = random.randint(*rand_factors) # Must be below 147\n",
    "    selected_factors = random.sample(selected_factors, num_factors)\n",
    "  else:\n",
    "    selected_factors = selected_factors\n",
    "\n",
    "  print(f\"[{len(selected_factors)}] Selected factors: {selected_factors}\")\n",
    "\n",
    "  # Selecting random Stocks\n",
    "  if rand_stocks:\n",
    "    num_stocks = random.randint(*rand_stocks) # Should be around 50-100\n",
    "    selected_stocks = random.sample(stock_permnos, num_stocks)\n",
    "  else:\n",
    "    selected_stocks = stock_permnos\n",
    "\n",
    "  print(f\"[{len(selected_stocks)}] Selected Stocks\")\n",
    "\n",
    "\n",
    "  extract_data = data[data['permno'].isin(selected_stocks)][['date', 'year', 'month', 'permno', 'stock_exret'] + selected_factors]\n",
    "\n",
    "  return selected_factors, extract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x101525xeT8e"
   },
   "source": [
    "<a name=\"2.5.1\"></a>\n",
    "### 2.5.1 - **Heterogeneous ensembles** approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1IoqzXDuxiO"
   },
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zhYqo-jeT8e",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We initially explored *Heterogeneous Ensembles*, a method highlighted in various research papers, which appeared promising for our needs.\n",
    "\n",
    "Using a **Hybrid Ensembles** approach for feature selection involves leveraging a subset of data through **Homogeneous Ensembles** and combining different algorithms via **Heterogeneous Ensembles**.\n",
    "\n",
    "- **Homogeneous Ensembles**: In this approach, the same feature selection method is applied across different subsets of training data.\n",
    "  >\"*In the homogeneous approach, the same feature selection method is used, but with different training data subsets.*\"\n",
    "  <div style=\"text-align:center\"><img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S1566253518303440-gr2.jpg\" width=\"70%\"></div>\n",
    "\n",
    "- **Heterogeneous Ensembles**: This method employs various feature selection techniques on the same training data, allowing for a more diverse selection process.\n",
    "  >\"*For the heterogeneous approach, a number of different feature selection methods, but over the same training data, are applied*\"\n",
    "  <div style=\"text-align:center\"><img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S1566253518303440-gr3.jpg\" width=\"70%\"></div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Next, we utilized the *Union* method to combine the selected features:\n",
    "\n",
    "> \"The union consists in combining all the features which have been selected by at least one of the feature selectors. Contrary to the intersection, it can lead to select even the whole set of features. This approach tends to produce better results than the intersection [41], but at the expense of a lower reduction in the number of the features.\"\n",
    "\n",
    "*Source: [Ensembles for feature selection: A review and future trends](https://www.sciencedirect.com/science/article/pii/S1566253518303440?casa_token=Gppypu5uSqgAAAAA:mYcUqLgqUqCJWRMOw85cOnnlCdUQiL6Y5cjvA1dv7riCTHEwBL9pLZNauKmEuO8un3rkPl1WdQ)*\n",
    "\n",
    "This structured approach allowed us to effectively harness the strengths of both homogeneous and heterogeneous ensembles for feature selection, ultimately enhancing our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwIssTa9yYOA"
   },
   "source": [
    "#### Wrap Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWahvjuVpiXa"
   },
   "outputs": [],
   "source": [
    "# 1. Filter Method: Pearson Correlation Analysis\n",
    "def correlation_selection(X, y, k=10):\n",
    "  cor = pd.DataFrame(X.corrwith(y)).abs()\n",
    "  selected_features = cor[:k].index.tolist()\n",
    "  return X[selected_features]\n",
    "\n",
    "# 2. Filter Method: Mutual Information\n",
    "def mutual_info_selection(X, y, k=10):\n",
    "  mi = mutual_info_regression(X, y)\n",
    "  mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "  selected_features = mi_series.nlargest(k).index.tolist()\n",
    "  return X[selected_features]\n",
    "\n",
    "# 3. Wrapper Method: Recursive Feature Elimination\n",
    "def rfe_selection(X, y, k=10):\n",
    "  estimator = XGBRegressor(random_state=42)\n",
    "  param_grid = {\n",
    "      'n_estimators': [5, 10],\n",
    "      'learning_rate': [0.01, 0.1, 0.2]\n",
    "  }\n",
    "  grid_search = GridSearchCV(estimator, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "  grid_search.fit(X, y)\n",
    "  selector = RFE(grid_search.best_estimator_, n_features_to_select=k, step=1)\n",
    "  selector = selector.fit(X, y)\n",
    "  selected_features = X.columns[selector.support_].tolist()\n",
    "  return X[selected_features]\n",
    "\n",
    "# 4. Embedded Method: Lasso\n",
    "def lasso_selection(X, y):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    lasso_cv = LassoCV(alphas=[0.001], cv=5)\n",
    "    lasso_cv.fit(X, y)\n",
    "    selected_features = X.columns[lasso_cv.coef_ != 0].tolist()\n",
    "    return X[selected_features]\n",
    "\n",
    "# 5. Embedded Method: Elastic Net\n",
    "def elastic_net_selection(X, y):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    enet_cv = ElasticNetCV(alphas=[0.001], l1_ratio=[0.1, 0.5, 0.9, 1], cv=5)\n",
    "    enet_cv.fit(X, y)\n",
    "    selected_features = X.columns[enet_cv.coef_ != 0].tolist()\n",
    "    return X[selected_features]\n",
    "\n",
    "# 6. Embedded Method: XGBoost Feature Importance\n",
    "def rf_importance_selection(X, y, k=10):\n",
    "    rf = XGBRegressor(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X, y)\n",
    "    # Get feature importances from the best XGBoost estimator\n",
    "    importances = pd.Series(grid_search.best_estimator_.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    selected_features = importances.nlargest(k).index.tolist()\n",
    "    return X[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(X, y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Train a simple XGBoost model\n",
    "  model = XGBRegressor(random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = model.predict(X_test)\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lO29UO5bIdi"
   },
   "source": [
    "#### Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJLIw3NfY1u7",
    "outputId": "c9b7b63a-4291-4029-cbac-d0aa9b40394f"
   },
   "outputs": [],
   "source": [
    "sample_factor, sample_data = load_and_extract_data(data, selected_factors=factor, rand_stocks=(50, 50))\n",
    "# factors_288_months, data_288_months = inputData(data_file=os.path.join(CLEAN_DATA_FOLDER 'data_288_months.csv'), factor_file=os.path.join(CLEAN_DATA_FOLDER, 'factor_288_months.csv'))\n",
    "# sample_factor, sample_data = load_and_extract_data(factors_288_months, data_288_months, rand_stocks=(50, 50))\n",
    "\n",
    "X, y = sample_data[sample_factor], sample_data['stock_exret']\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zu6BvaHgRHqE",
    "outputId": "df47d55c-9996-4545-f25e-796ef3a3438e"
   },
   "outputs": [],
   "source": [
    "# Record the start time for the entire process\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Function to time each feature selection method\n",
    "def time_feature_selection(method_name, method, *args):\n",
    "    start = datetime.datetime.now()\n",
    "    result = method(*args)\n",
    "    end = datetime.datetime.now()\n",
    "    duration = end - start\n",
    "\n",
    "    # Convert duration to seconds\n",
    "    total_seconds = duration.total_seconds()\n",
    "    minutes = int(total_seconds // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "\n",
    "    # Format minutes and seconds as MM:SS\n",
    "    formatted_time = f\"{minutes:02}:{seconds:02}\"\n",
    "    print(f\"{method_name} completed in: {formatted_time}\")\n",
    "    return result\n",
    "\n",
    "# Apply feature selection methods with timing\n",
    "X_corr = time_feature_selection(\"Correlation selection\", correlation_selection, X, y)\n",
    "X_mi = time_feature_selection(\"Mutual information selection\", mutual_info_selection, X, y)\n",
    "X_rfe = time_feature_selection(\"RFE selection\", rfe_selection, X, y)\n",
    "X_lasso = time_feature_selection(\"Lasso selection\", lasso_selection, X, y)\n",
    "X_enet = time_feature_selection(\"Elastic Net selection\", elastic_net_selection, X, y)\n",
    "X_rf = time_feature_selection(\"XGBoost feature importance selection\", rf_importance_selection, X, y)\n",
    "\n",
    "# Record the end time for the entire process\n",
    "end_time = datetime.datetime.now()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "# Convert total duration to minutes and seconds\n",
    "total_seconds = total_duration.total_seconds()\n",
    "total_minutes = int(total_seconds // 60)\n",
    "total_seconds = int(total_seconds % 60)\n",
    "\n",
    "# Format total running time as MM:SS\n",
    "total_formatted_time = f\"{total_minutes:02}:{total_seconds:02}\"\n",
    "print(f\"Total running time for all methods: {total_formatted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeXCJvxNz9ow",
    "outputId": "e1d1da07-5006-44fe-f496-81ce3fe7841d"
   },
   "outputs": [],
   "source": [
    "feature_selection_methods = {\n",
    "    'Original': X,\n",
    "    'Correlation': X_corr,\n",
    "    'Mutual Information': X_mi,\n",
    "    'RFE': X_rfe,\n",
    "    'Lasso': X_lasso,\n",
    "    'Elastic Net': X_enet,\n",
    "    'XGBoost Feature Importance': X_rf,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate each method\n",
    "for method_name, X_selected in feature_selection_methods.items():\n",
    "    if X_selected.shape[1] > 0:\n",
    "        results.append([method_name, evaluate_model(X_selected, y)])\n",
    "    else:\n",
    "        results.append([method_name, float('inf')])\n",
    "\n",
    "results = pd.DataFrame(results, columns=['Methods', 'MSE'])\n",
    "\n",
    "# Plotting results\n",
    "results.plot(\n",
    "    kind='bar',\n",
    "    x='Methods',\n",
    "    y='MSE',\n",
    "    title='MSE for Different Feature Selection Methods',\n",
    "    ylabel='Mean Squared Error',\n",
    "    rot=45,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "features_df = pd.DataFrame(dict([(k, pd.Series(v.columns)) for k, v in list(feature_selection_methods.items())[1:]]))\n",
    "\n",
    "print(\"\\nSelected Features from Each Method:\")\n",
    "display(features_df)\n",
    "\n",
    "# Union all features from all methods\n",
    "all_selected_features = pd.concat([features_df[col] for col in features_df.columns]).dropna().drop_duplicates().reset_index(drop=True)\n",
    "print(f'Total Selected Features: {len(all_selected_features)} {all_selected_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tu7kMaX1SUq6",
    "outputId": "ec9609cd-6723-4329-caff-f45e88b736c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_factor_heterogeneous, selected_data_heterogeneous = load_and_extract_data(data, selected_factors=all_selected_features.tolist())\n",
    "# outputData(selected_factor, selected_data, data_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_data.csv'), factor_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_factor.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTUAFtdyeT8g"
   },
   "source": [
    "<a name=\"2.5.2\"></a>\n",
    "### 2.5.2 - RFE with `n_estimators=500`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgWJygafeT8g"
   },
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp3azLDjeT8g"
   },
   "source": [
    "Using **Recursive Feature Elimination (RFE)** with an **XGBoost regressor** is an effective for identifying the most important features when predicting stock excess returns. RFE is a popular feature selection algorithm that works by recursively removing the least important features based on the model's performance, allowing the model to focus on the most relevant information.\n",
    "\n",
    "The process ensures that the model is trained on a well-structured with `500 estimators` and relevant subset of the data, which can enhance its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRTFuj9_eT8h"
   },
   "source": [
    "#### Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_sFYl1TeT8h",
    "outputId": "fe719fe9-839a-47b2-9d5f-d1296e4d6a83"
   },
   "outputs": [],
   "source": [
    "# Features and target variable selection\n",
    "X = data.drop(columns=['stock_exret', 'comp_name', 'date', 'permno', 'year', 'month'])\n",
    "y = data['stock_exret']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features using RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled arrays back to DataFrames to keep column names\n",
    "X_train_scaled_data = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled_data = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Feature selection using RFE\n",
    "n_features_to_select = 50  # Specify the number of features to select\n",
    "\n",
    "# Initialize the estimator for RFE with correct XGBoost parameters\n",
    "estimator = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,    # Consider setting a specific depth if needed for control\n",
    "    learning_rate=0.1,  # XGBoost uses learning_rate (analogous to shrinkage)\n",
    "    subsample=0.8,      # Control the fraction of samples used\n",
    "    colsample_bytree=0.8,  # Control the fraction of features used\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Initialize RFE with step parameter for faster elimination\n",
    "selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=5)\n",
    "selector = selector.fit(X_train_scaled_data, y_train)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_factor_RFE = X_train_scaled_data.columns[selector.support_].tolist()\n",
    "print(f\"Top {n_features_to_select} features selected using RFE:\")\n",
    "print(selected_factor_RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOsDUC8weT8h",
    "outputId": "fe96978f-102c-4519-a6fc-3b16e8ae5a90"
   },
   "outputs": [],
   "source": [
    "selected_factor_RFE, selected_data_RFE = load_and_extract_data(data, selected_factors=selected_factor_RFE)\n",
    "# outputData(selected_factor, selected_data, data_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_data.csv'), factor_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_factor.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zszb4X1TeT8i"
   },
   "source": [
    "<a name=\"2.5.3\"></a>\n",
    "### 2.5.3 - Evaluation for Each Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XgOoZaceT8i",
    "outputId": "6aa8dfeb-3d4f-4a43-ffe2-d7c8c1984e71"
   },
   "outputs": [],
   "source": [
    "# Function to calculate R-squared for a given dataset\n",
    "def calculate_r_squared(selected_data):\n",
    "    # Features and target variable selection\n",
    "    X = selected_data.drop(columns=['stock_exret', 'date', 'permno', 'year', 'month'])\n",
    "    y = selected_data['stock_exret']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features using RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize the estimator for RFE with correct XGBoost parameters\n",
    "    estimator = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,    # Consider setting a specific depth if needed for control\n",
    "        learning_rate=0.1,  # XGBoost uses learning_rate (analogous to shrinkage)\n",
    "        subsample=0.8,      # Control the fraction of samples used\n",
    "        colsample_bytree=0.8,  # Control the fraction of features used\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the XGBoost model on the selected features\n",
    "    estimator.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = estimator.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate and return the R-squared score\n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    return r_squared\n",
    "\n",
    "# Assuming selected_factor_RFE and selected_data_RFE are your DataFrames\n",
    "r_squared_heterogeneous = calculate_r_squared(selected_data_heterogeneous)\n",
    "r_squared_RFE = calculate_r_squared(selected_data_RFE)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Heterogeneous', 'RFE'],\n",
    "    'R_squared': [r_squared_heterogeneous, r_squared_RFE]\n",
    "})\n",
    "# Determine the best method based on R-squared\n",
    "best_method = results_df.loc[results_df['R_squared'].idxmax()]\n",
    "\n",
    "# Display the results\n",
    "display(results_df)\n",
    "print(f'Best R_squared: {best_method.Method}:{best_method.R_squared}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shLh36zgeT8i",
    "outputId": "0c8ced5c-a01b-4ccd-efdf-40006bf60960"
   },
   "outputs": [],
   "source": [
    "# Determine the best method based on R-squared\n",
    "best_method = results_df.loc[results_df['R_squared'].idxmax()]\n",
    "\n",
    "# Save the best to a file\n",
    "if best_method['Method'] == 'selected_factor_RFE':\n",
    "    outputData(selected_factor_RFE, selected_data_RFE, data_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_data.csv'), factor_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_factor.csv'))\n",
    "else:\n",
    "    outputData(selected_factor_heterogeneous, selected_data_heterogeneous, data_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_data.csv'), factor_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_factor.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMniloyseT8i"
   },
   "source": [
    "#### Read Extracted Data and Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdXvBXUleT8j",
    "outputId": "28485113-fef2-48ff-aa1f-7e7632e6d863"
   },
   "outputs": [],
   "source": [
    "selected_factor, selected_data = inputData(data_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_data.csv'), factor_file=os.path.join(CLEAN_DATA_FOLDER, 'selected_factor.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6POTSuxiwTIj"
   },
   "source": [
    "<a name=\"2.6\"></a>\n",
    "### 2.6 - Ramdom sample (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn54gCLDwTIj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selected_factors, selected_stocks = extract_data(rand_factors=(10, 30), rand_stocks=(50, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYDBKJrtwTIl"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "CnNvdokuwTIl",
    "outputId": "df494bb3-0dfd-4367-f15e-77adc6258a0b"
   },
   "outputs": [],
   "source": [
    "%run predict_data.py --data=selected_data.csv --factor=selected_factor.csv --work_dir={CLEAN_DATA_FOLDER} --output_dir={PREDICTED_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sim3vYQweT8k"
   },
   "outputs": [],
   "source": [
    "# Old Models\n",
    "\n",
    "# %run penalized_linear_hackathon.py --data=selected_data.csv --factor=selected_factor.csv --work_dir={CLEAN_DATA_FOLDER} --output_dir={PREDICTED_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ni_ZvCklwTIm"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "EQ0udgru2hYF",
    "outputId": "ab1041b1-d2ab-42f2-bbd6-56a264f2c391"
   },
   "outputs": [],
   "source": [
    "%run portfolio_analysis_hackathon.py --predicted=output.csv --model=xgb --mkt_ind={ASSET_MKT_IND_PATH} --work_dir={PREDICTED_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOUISQW_efFp"
   },
   "outputs": [],
   "source": [
    "# for model_name in ['ols', 'lasso', 'ridge', 'en', 'xgb']:\n",
    "#   print(f'\\n\\n{model_name}: ')\n",
    "# %run portfolio_analysis_hackathon.py --predicted='output.csv' --model='{model_name}' --mkt_ind={ASSET_MKT_IND_PATH} --work_dir='{PREDICTED_FOLDER}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UlErcqizOG2"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Cumulative Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tjUnvpBeT8l"
   },
   "source": [
    "#### Wrap Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef5IpSTCeT8l"
   },
   "outputs": [],
   "source": [
    "# Create mixed strategy with long and short positions\n",
    "def mixed_strategy(df, n_stocks, long_short_split=0.7, model=\"xgb\"):\n",
    "    long_n = int(n_stocks * long_short_split)  # Number of long positions\n",
    "    short_n = n_stocks - long_n  # Number of short positions\n",
    "    top_n = df.nlargest(long_n, model)  # Top long_n for long\n",
    "    bottom_n = df.nsmallest(short_n, model)  # Bottom short_n for short\n",
    "    return top_n, bottom_n\n",
    "\n",
    "def create_portfolios(pred, n_stocks):\n",
    "    monthly_portfolios = []\n",
    "    for _, group in pred.groupby(['date']):\n",
    "        long_stocks, short_stocks = mixed_strategy(group, n_stocks=n_stocks, long_short_split=0.7)\n",
    "        long_stocks['position'] = 1  # Long position\n",
    "        short_stocks['position'] = -1  # Short position\n",
    "        combined = pd.concat([long_stocks, short_stocks])\n",
    "        monthly_portfolios.append(combined)\n",
    "\n",
    "    combined_portfolio = pd.concat(monthly_portfolios)\n",
    "\n",
    "    return combined_portfolio\n",
    "\n",
    "def compute_weighted_return(portfolio):\n",
    "    # Combine the monthly portfolios\n",
    "    portfolio['weighted_return'] = portfolio['stock_exret'] * portfolio['position']\n",
    "    # Aggregate and calculate the performance\n",
    "    monthly_performance = portfolio.groupby(['year', 'month', 'date']).agg({'weighted_return': 'mean'}).reset_index()\n",
    "\n",
    "    return monthly_performance\n",
    "\n",
    "# Iterate through numbers from 50 to 100 and find the one with the highest Sharpe Ratio\n",
    "def find_best_number_of_portfolios(pred, from_=50, to=100):\n",
    "    best_sharpe = -np.inf  # Initialize best Sharpe ratio as negative infinity\n",
    "    best_n_stocks = None  # Initialize best number of stocks\n",
    "\n",
    "    print(f'Finding best number of stocks... ', end='')\n",
    "\n",
    "    for n_stocks in range(from_, to+1):  # Loop through numbers from 50 to 100\n",
    "        mixed_portfolio = create_portfolios(pred, n_stocks)\n",
    "        monthly_performance = compute_weighted_return(mixed_portfolio)\n",
    "\n",
    "        # Calculate the Sharpe Ratio for this portfolio\n",
    "        sharpe = calculate_sharpe_ratio(monthly_performance)\n",
    "\n",
    "        # Check if this is the highest Sharpe ratio\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe = sharpe\n",
    "            best_n_stocks = n_stocks\n",
    "\n",
    "    print(f\"| Best number of stocks:{best_n_stocks} | Sharpe Ratio:{best_sharpe}\")\n",
    "\n",
    "    return best_n_stocks, best_sharpe\n",
    "\n",
    "# Calculate the Sharpe Ratio for a given portfolio\n",
    "def calculate_sharpe_ratio(portfolio):\n",
    "    mean_return = portfolio['weighted_return'].mean()\n",
    "    std_dev = portfolio['weighted_return'].std()\n",
    "    sharpe = mean_return / std_dev * np.sqrt(12)  # Annualized Sharpe ratio\n",
    "    return sharpe\n",
    "\n",
    "# Calculate the annualized return\n",
    "def annualized_return(monthly_returns):\n",
    "    compounded_growth = (1 + monthly_returns).prod()\n",
    "    n_months = len(monthly_returns)\n",
    "    annual_return = compounded_growth**(12 / n_months) - 1\n",
    "    return annual_return\n",
    "\n",
    "# Calculate the annualized standard deviation\n",
    "def annualized_std(monthly_returns):\n",
    "    monthly_std = monthly_returns.std()\n",
    "    annualized_std = monthly_std * np.sqrt(12)\n",
    "    return annualized_std\n",
    "\n",
    "# Plot Cumulative Performance of Mixed Strategy Portfolio vs S&P 500\n",
    "def plot_cumulative(monthly_performance):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(monthly_performance['year'] + monthly_performance['month']/12, monthly_performance['cumulative_portfolio'], label=\"Mixed Strategy Portfolio\")\n",
    "    plt.plot(monthly_performance['year'] + monthly_performance['month']/12, monthly_performance['cumulative_sp500'], label=\"S&P 500\", linestyle=\"--\")\n",
    "    plt.title('Cumulative Performance: Mixed Strategy vs. S&P 500 (2010â€“2023)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Cumulative Returns')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ReSCTBeT8m"
   },
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### 5.1 - Plot Cumulative Performance: Mixed Strategy vs. S&P 500 (2010â€“2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfuu69Fx7K63",
    "outputId": "3610fa12-91e3-4cb9-b1e5-451412c58496"
   },
   "outputs": [],
   "source": [
    "mkt = pd.read_csv(ASSET_MKT_IND_PATH)\n",
    "pred = pd.read_csv(OUTPUT_PREDICTS_PATH, parse_dates=[\"date\"])\n",
    "\n",
    "# Filter for OOS testing period: 01/2010 to 12/2023\n",
    "pred = pred[(pred['date'] >= '2010-01-01') & (pred['date'] <= '2023-12-31')]\n",
    "\n",
    "# Select model (e.g., 'xgb')\n",
    "model = \"xgb\"\n",
    "\n",
    "best_n_stocks, best_sharpe = find_best_number_of_portfolios(pred)\n",
    "\n",
    "# Now create the final portfolio using the best number of stocks\n",
    "final_portfolio = create_portfolios(pred, best_n_stocks)\n",
    "\n",
    "# Calculate the performance\n",
    "monthly_performance = compute_weighted_return(final_portfolio)\n",
    "print(f\"Final portfolio Sharpe ratio: {calculate_sharpe_ratio(monthly_performance)}\")\n",
    "\n",
    "# Calculate and print the overall annualized return and standard deviation\n",
    "portfolio_annual_return = annualized_return(monthly_performance['weighted_return'])\n",
    "portfolio_annual_std = annualized_std(monthly_performance['weighted_return'])\n",
    "\n",
    "print(f\"Portfolio Annualized Return: {portfolio_annual_return:.4f}\")\n",
    "print(f\"Portfolio Annualized Standard Deviation: {portfolio_annual_std:.4f}\")\n",
    "\n",
    "# Merge with S&P 500 returns for comparison\n",
    "\n",
    "monthly_performance = monthly_performance.merge(mkt, how=\"inner\", on=[\"year\", \"month\"])\n",
    "monthly_performance[\"cumulative_portfolio\"] = (1 + monthly_performance[\"weighted_return\"]).cumprod()\n",
    "monthly_performance[\"cumulative_sp500\"] = (1 + monthly_performance[\"sp_ret\"]).cumprod()\n",
    "\n",
    "#############\n",
    "plot_cumulative(monthly_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmelyY0YeT8m"
   },
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### 5.2 - 10 most held stocks in our portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-KNrE-m7Qfw",
    "outputId": "166e5b6d-01d5-42b5-8c32-d69596bb858a"
   },
   "outputs": [],
   "source": [
    "n_top = 10\n",
    "\n",
    "# Count the number of appearances of each stock (permno) across all months in the final portfolio\n",
    "stock_counts = final_portfolio['permno'].value_counts().rename('frequency').reset_index()\n",
    "# Get the top 10 most frequently held stocks\n",
    "top_n_stocks = stock_counts.head(n_top).merge(raw.drop_duplicates(subset='permno')[['permno', 'comp_name']], on='permno', how='left')\n",
    "top_n_stocks.plot(kind='barh',\n",
    "                   x='comp_name',\n",
    "                   y='frequency',\n",
    "                   color='skyblue',\n",
    "                  xlabel='Frequency',\n",
    "                  ylabel='Company',\n",
    "                  title=f'Top {n_top} Held Stocks with Merged Company Names and Frequencies').invert_yaxis()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jHuJqfOfugsM",
    "ONc_BMHvP36x",
    "gM4ROZm_QAYy",
    "li6O9XksfsFS",
    "Q_xyF0lEsF9S",
    "O1IoqzXDuxiO",
    "MwIssTa9yYOA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_investment_allocation",
   "language": "python",
   "name": ".venv_investment_allocation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
